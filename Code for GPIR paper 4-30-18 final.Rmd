---
title: "Code for McClanahan et al. GPIR Submission"
author: "written by Kaylene McClanahan"
date: "April 30, 2018"
output: word_document
---
## How to Use this Document:
This document contains the code that we used to conduct our analyses (for both the main text and the supplemental materials). It contains both chunks of R code and annotation which explains what the coding is doing. In addition to the explanation between the code chunks, there is line-by-line annotation (denoted with a #) is embedded within the code. For example:

```{R}
sample <- "this is an example line of R code"
#this is a comment
```
In addition to containing the code used, this document also contains all of the *results* that are generated by running this code in R. For example, see the expression below:
```{R}
#when we type this:
3*4

#the result (12) is also generated.
```
If you'd like to run these analyses in R yourself, you can also open the .Rmd version of this file in R. This will enable you to run all of the code in this document (please note that some file directories will need to be changed and package installation may need to be altered depending on what is already installed on your computer).  

The layout of this document is roughly equivalent to the layout of the results in the paper. 
However, as with most projects, our data required some cleaning before it was ready for analysis. All data wrangling and preliminary analyses for this paper were conducted in R and are included in this document. Thus, for each study, there are sections of code that are used for cleaning/compiling, etc. Beyond the code for cleaning the data, we have divided the core analyses of the paper into 16 "chunks" for organizational purposes. Each chunk represents an essential analysis in the paper.  For example, each of the three studies has four chunks: demographics, scales, manipulation check, and 3-way interaction (as this follows that layout of the paper; Study 1 is chunks 1-4; Study 2 is chunks 4-8; Study 3 is chunk 9-12). There are two chunks for the combined data: demographics and 3-way interaction (chunk 12-13). Finally, there is a chunk of code to produce the table of descriptive statistics and a chunk of code to analyze our post-test for (chunks 15 and 16, respectively).

Following the code for the main text is all the code used to produce the supplemental materials. The code for this section follows the organization of the supplemental materials. 

## Preparing for Analyses:
In addition to base R, we used several different packages to help us easily perform specific functions within R. These packages are installed and loaded below. The function that each package serves in the paper is listed in the in-line comments below:

```{r eval=FALSE}
#installing necessary packages
install.packages('dplyr') #this package makes it easier to sort, filter, and manipulate data
install.packages('psych') #this packages makes it easy to calculate reliabilties (alphas)
install.packages('lsr') #this package allows us to calculate the partial eta-squared on our ANOVA
install.packages('broom') #this package allows us to easily turn regression results into a data frame
install.packages('Hmisc') #I use this package for my correlation tables, but I do'nt library it until I need it because it changes some defaults in a way that I don't like
install.packages("devtools") #this allows us to install packages from github (below)
devtools::install_github("jeff-hughes/reghelper") #this makes it easy to get the simple slopes from an interaction
```
After installing these packages, we load them into the workspace:  
```{r message = FALSE}
library(lsr)
library(dplyr)
library(psych)
library(broom)
library(reghelper)

```
Now we're ready to look at the data, which I laid out study by study:

#Study 1 Analyses
Before doing anything with the Study 1 data, we need to import the data into R. We start with the .csv files we downloaded from Qualtrics. These .csv files come from Qualtrics with a variable labels row; this causes problems for R, so I removed this row from the file prior to importing. There are also 3 columns which were removed from the dataset. These columns represented write-in questions that probed for participants' suspicion (given that our study relied on manipulation). I have removed these questions because they caused problems during upload (.csv files use commas for separating cells and the comments frequently contained commas, creating confusion).
  
Study 1 was actually comprised of Study 1a and Study 1b so there will be two files to upload here. If running these analyses on a different computer, the file directory below will need to be changed to reflect the location of the .csv files.
```{r}
#Study 1a
study1a <- read.csv(file="study 1a no labels.csv")

#Study 1b
study1b <- read.csv(file="study 1b no labels.csv")

```
We now have two data frames, `study1a` and `study1b`. These data frames comprise everyone who took our Study 1 survey. However, we are only interested in investigating participants who completed the study and consented to letting us use our data:
```{r}
#Study 1a
nrow(study1a) # we start with 379 observations in Study 1a, some of which are incomplete 

study1a <- study1a %>%
 filter(Q65 == 1) #this filters out people who didn't complete the study or didn't let us use data

nrow(study1a) #there are now 327 people in Study 1a


#Study 1b
nrow(study1b) #451 cases in Study 1b to start with

study1b <- study1b %>%
  filter(Q65 == 1) #filtering out people who didn't complete study or didn't consent to letting us use their data

nrow(study1b) #there are now 402 people in Study 1b
```
We now have two data frames with all the complete observations.   
  
##Cleaning Studies 1a and 1b  
There are a number of things we need to do get this raw data into a form where we can run analyses on it. We will do most of this after combining Study 1a and Study 1b to be one dataset, but there is one key difference between these datasets that needs to be changed before the merge: their condition variables. 
  
For Study 1a, which has three condition, we will create a numeric condition variable which is coded as follows:  
Reputable condition = 3  
Neutral condition = 2  
Disreputable condition = 1  
  
Here is the code:
```{r}
#creating the condition variable from qualtric's randomization variable; high numbers = better reputation target
study1a$condition[study1a$DO.BR.FL_15 == "target does something good"] <- 3
study1a$condition[study1a$DO.BR.FL_15 == "target does something neutral"] <- 2
study1a$condition[study1a$DO.BR.FL_15 == "target does something bad"] <- 1

#checking to make sure this worked:
#each condition should have approximately the same  N in each, and status should be highest in cond 3:
study1a %>% 
  group_by(condition) %>%
  summarise(N = n(), "Mean Status" = mean(status)) #this worked as planned
```
Now we do a similar thing for Study 1b, but there are only 2 conditions here, so we used contrast coding (high status = 1, low status = -1):  
```{r}
study1b$cond[study1b$DO.BR.FL_15 == "target does something good"] <- 1
study1b$cond[study1b$DO.BR.FL_15 == "target does something bad"] <- -1

#checking to make sure that conditions are approximately equal in size, and status is high in condition 3
study1b %>%
  group_by(cond) %>%
  summarise(N = n(), "Mean Status" = mean(status))
```
##Merging Studies 1a and 1b
Now that the condition variables are created, we can merge Study 1a and Study 1b. Before merging, we will create a new `study1a` dataframe that only has the reputable and disreputable condition, which need to be contrast coded. The `study1a` dataset will now only have two conditions, but we will make a dataset called `study1afull` that retains the neutral condition (this will be used for analyses in the supplemental materials).  
``` {r}
#filtering out the neutral condition
study1afull <- study1a
study1a <- study1a %>%
  filter(condition == 3 | condition == 1) #new dataframe with just cond 1 and 3

#creating a "cond" variable in study1a which is bad/good condition but contrast code (to match study 1b)
study1a$cond = study1a$cond - 2 #subtracting 2 to make it a contrast code (from 1 and 3)

#making the cond variables factors in each dataframe
study1a$cond = as.factor(study1a$cond) 
study1b$cond = as.factor(study1b$cond)

#checking to make sure it worked
summary(study1a$cond) #condition N's are the same as the original variable
summary(study1b$cond) #condition N's are the same as the original variable
```

The age variable in Study 1a has some bizarre entries that will throw an error during the merge if not corrected. Additionally, R has somewhat stringent rules about variables of different types are merged, so the age variable needs to be changed from a factor to an integer. (You need to make it a character variable first to make this work correctly.)
```{r}
#manually recode errors
study1a$demo_age[study1a$demo_age == "OH"] <- NA
study1a$demo_age[study1a$demo_age == 224] <- NA

#factor -> character -> integer
study1a$demo_age <- as.integer(as.character(study1a$demo_age)) 
class(study1a$demo_age) #class is now integer
```
All of the relevant variables are ready to merge, so let's make smaller dataframes that only contain the variables we will need for our analyses:
```{r}
#selecting variables for the merge from study 1a
study1amerge <- study1a %>%
  select(V1,demo_age, demo_gend, starts_with("ethnic"), attn, starts_with("SDO"), status, 
         attn2, starts_with("hypo"), cond) #study1a is ready to go

#selecting variables for the merge from study 1b
study1bmerge <- study1b %>%
  rename(hypo_1 = Q78, hypo_2 = Q80) %>% #these variables aren't named in this study for some reason
  select(V1,demo_age, demo_gend, starts_with("ethnic"), attn, starts_with("SDO"), status, 
         attn2, starts_with("hypo"), cond) 
```
Now we are ready to merge!  We will ask R to create a new variable, `source1aor1b` in our new dataframe, which will tell us which study (1a or 1b) the observation came from. 
```{r warning=F}
#merging
study1 <- bind_rows("study1a" = study1amerge, "study1b" = study1bmerge, .id = "source1aor1b")
class(study1$cond)

#checking the merge... are the N's correct?
nrow(study1) #621 rows, that's equal to 106 + 113 + 197 +205 
summary(study1$cond) #checking the conditions--they look correct (303 bad, 318 good)
study1 %>%
  group_by(cond) %>%
  summarise(mean(status)) #status is still higher in the reputable condition, everything looks good
```
##Cleaning Study 1 Data
Now that we have a single dataset, we can go about prepping it for analyses. Specifically, we will need to create some scales (SDO, ethnic identification, formativeness and categorization). We also need to change ethnicity and gender into factors.
```{r}
#_____________
#SDO
#reverse code items:
study1$SDO7_5 <- 8-study1$SDO7_5
study1$SDO7_6 <- 8-study1$SDO7_6
study1$SDO7_7 <- 8-study1$SDO7_7
study1$SDO7_8 <- 8-study1$SDO7_8
study1$SDO7_13 <- 8-study1$SDO7_13
study1$SDO7_14 <- 8-study1$SDO7_14
study1$SDO7_15 <- 8-study1$SDO7_15
study1$SDO7_16 <- 8-study1$SDO7_16

#create the scale
study1$sdo <- study1 %>%
  select(starts_with("SDO7_")) %>%
  rowMeans(na.rm = TRUE) 

#checking for NAs (empty cases)--there shouldn't really be any because everyone should have at least some SDO values in this 
anyNA(study1$sdo) #false means no NAs

#_____________  
#ETHNIC ID 
#we want to use ethnic_id2 & ethnic_id3; those are the only ones in all studies.
study1$ethnic <- rowMeans(subset(study1, select = c(ethnic_id2, ethnic_id3)), na.rm = TRUE) #making scale
summary(study1$ethnic) #no NAs, great


#_____________
#FORMATIVENESS & CATEGORIZATION
#hypo 1 and 2 are categorization; hypo 4-5, 11-12 are formativeness (but 11 and 12 are not included in Study 1a so we are not looking at them in this study)
study1$form <- rowMeans(subset(study1, select = c(hypo_4, hypo_5), na.rm = TRUE))
study1$cat <- rowMeans(subset(study1, select = c(hypo_1, hypo_2), na.rm = TRUE))
summary(study1$form) #no NAs, great
summary(study1$cat) #1 NA, that's fine


#_____________
#MAKING ETHNIC ID & GENDER FACTORS
study1$ethnic_id <- factor(study1$ethnic_id, labels = c("Black/African American", "Asian/Asian American/Pacific Islander", "White", "Latino/Hispanic American", "Middle Eastern/Arab American","Native American", "Biracial/Mixed race (please specify)", "Other (please specify)"))

study1$demo_gend2 <- factor(study1$demo_gend, labels = c("male", "female"))
```
Great! The data is now ready for analyses!  

##Analysis Chunk 1: Study 1 Demographics  

First, let's filter the dataset until we have only the participants we want, then look at the *N* and the demographics:

```{r}
#Checking the N from each study

#first, how many were in study 1a (all conditions?)
nrow(study1afull)

#and in just the reputable condition?
study1 %>%
  group_by(source1aor1b) %>%
  summarise(n()) #219 in study 1a, 402 in study 1b:

nrow(study1) #making a total of 621 people in study 1


#how many white participants?
study1 %>% 
  filter(ethnic_id == "White") %>%
  nrow() #470 White participants

#selecting white participants who completed the attention checks; final sample
study1f <- study1 %>%
  filter(ethnic_id == "White" & attn ==5 & attn2 == 3) #making FINAL study 1
nrow(study1f) #final sample is 459
470-459 #11 people didn't pass attention checks


#__DEMOGRAPHICS__
#ages
#checking for outliers
study1f$demo_age[study1f$demo_age > 100 | study1f$demo_age < 18] #there is one outlier (335 years old?)
#removing outlier
study1f$demo_age[study1f$demo_age > 100 | study1f$demo_age < 18] <- NA 

#calulating mean age
describe(study1f$demo_age, na.rm = TRUE) #mean age is 35.90; sd = 12.08

# calculating gender distribution
summary(study1f$demo_gend2) #207 males, 252 females; 
252/(252+207) #54.9% females
```

##Analysis Chunk 2: Study 1 Scales  
Now we'll check to make sure the scales are robust:


```{r}
#ethnic id correlation
cor.test(study1f$ethnic_id2, study1f$ethnic_id3) #r = .71 p < .001

#sdo reliability
psych::alpha(select(study1f, starts_with("SDO7_")))  #alpha is .95

#formativeness correlation
cor.test(study1f$hypo_4, study1f$hypo_5) #r = .66 p < .001
```

##Analysis Chunk 3: Study 1 Manipulation Check  
The next step is to run a manipulation check---Did participants in the reputable condition actually perceive the target as being more reputable? We could run a t-test but an ANOVA produces equivalent results and allows us to look at the partial eta squared.
```{r}
s1manip <- aov(with(study1f, status ~ cond)) #create the model
summary(s1manip) #see results from the model
etaSquared(s1manip) #calculates the effect size; this is where the lsr package comes into play!

#getting the mean/sd for each condition
study1f %>%
  group_by(cond) %>%
  summarise(mean = mean(status), sd = sd(status))
```

##Analysis Chunk 4: Study 1 Three-Way Interaction
That concludes all of our preliminary data wrangling/analyses. To test our hypotheses, we ran a 3-way interaction between SDO, ethnic identification, and condition on our formativeness variable. Before running this model, we mean-centered our continuous variables and ensured our dummy-coded condition variable was a factor:
```{R}
study1f$sdocent <- scale(study1f$sdo, center = TRUE, scale = FALSE)
study1f$ethcent <- scale(study1f$ethnic, center = TRUE, scale = FALSE)
study1f$sdocent <- study1f$sdocent[,1]
study1f$ethcent <- study1f$ethcent[,1]
study1f$cond2 <- as.factor(study1f$cond)

library(dplyr)

study1f %>%
  group_by(cond) %>%
  summarise(status = mean(status, na.rm = T), form = mean(form, na.rm = T), cat = mean(cat, na.rm = T))

```
Now we are ready to build the model. In R, after you build a model, you need to run the `summary()` function on it in order to view the results. You also need to ask for 95% confidence intervals with a separate function.
```{r}
s1model <- lm(form~sdocent*ethcent*cond2, data=study1f)

summary(s1model)

confint(s1model) #this gives us the 95% confidence intervals

```
This gives us the information that we need, but it is still a little difficult to wade through. To make it easier, we can make it all into a little dataframe that has only the information
we need for the manuscript (with all the numbers appropriately rounded).  

To do this, I'm using the `tidy()` function from the `broom` package to pull regression results into a dataframe. We can do the same thing with confidence intervals. 
```{r}
s1modeldf <- tidy(s1model)
s1cidf <- tidy(confint(s1model))
```
Then we can bind these two dataframes and change the variable names so they are all one word (and thus easier to work with). 
```{r}
names(s1cidf) <- c("parameter", "lowerci", "upperci")
s1modeldf <- bind_cols(s1modeldf, s1cidf) #now I have a dataframe with parameters, estimates, and confidence intercepts
names(s1modeldf) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci") #and renaming
```
Finally, we can reduce this to the columns we need for the paper while rounding to the appropriate number of decimals for the paper.
```{r}
s1modeldf %>%
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need
```
Great!  Now we need to get the simple slopes. We'll use the `simple_slopes()` function from the `reghelper` package.  
```{r}
s1simpslope <- simple_slopes(s1model)
print(s1simpslope)
```
This is great, but this function doesn't produce the 95% confidence intervals for the coefficients.  In order to manually compute the confidence interval, we will use the following formula:  
 95% confidence interval = coefficient ± 2(std error)
While we're getting the CIs, we'll also restructure the results so they are easy to put it into the paper.  

First, we will create a reduced dataframe with only the rows (parameters) that we are interested in for the paper. We'll also change the names of the columns while we are at it so they are all one word (easier to work with)
```{r}
s1simpsloper <- s1simpslope[c(8, 2, 17, 11, 6, 4, 20, 14), ] 
names(s1simpsloper)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p")

```
Good, now we create two columns for lower and upper confidence intervals.
We'll also do a few other things to make it easier to copy these values into the paper:    
 1. We'll round the columns in accordance with APA  
 2. We'll make a new parameter column that will make it easy to see which parameter we are looking at  
 3. We'll make select only the columns that we need for the the paper  
```{r}
s1simpsloper %>% 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_eth", "cond_at_low_eth", "eth_in_high_cond",
                       "eth_in_low_cond", "cond_at_high_sdo","cond_at_low_sdo",
                       "sdo_in_high_cond","sdo_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```
And those are our results!


#Study 2 Analyses
Just like Study 1, we start Study 2 by importing the data from a .csv file. This .csv is raw from Qualtrics (with the exception of the suspicion questions and variable labels being removed).
```{r}
study2 <- read.csv("study 2 school shooting no labels.csv")
```
Then we filter off the incomplete cases and any case where the participant did not consent to letting us use their data:
```{r}

#start by filtering out the cases where people didn't consent to letting us use their data
nrow(study2) # we start with 573 lines of data
study2 <- study2 %>%
  filter(data_yn == 1) #this filters out people who didn't complete the study or didn't let us use data
nrow(study2) #there are now 501 people in this sample 
```
##Cleaning Study 2 Data
Now that we have our full dataset ready, we can clean the data and prep it for analyses. Let's start by creating the condition variable, which again will be contrast coded:
```{r}
#creating the condition variable; info is stored in both DO-BR-FL_127 and DO-BR-FL_12
study2$cond[study2$DO.BR.FL_127 == "high Status"] <- 1
study2$cond[study2$DO.BR.FL_129 == "high Status"] <- 1
study2$cond[study2$DO.BR.FL_127 == "low status"] <- -1
study2$cond[study2$DO.BR.FL_129 == "low status"] <- -1
class(study2$cond)

#checking the Ns and mean status in each condition
study2 %>%
  group_by(cond) %>%
  summarise(N = n(), "Mean Status" = mean(status, na.rm = TRUE)) #239 bad cond; 262 good cond; perceptions of status are higher in good condition; (i.e., it's correctly coded)
```
Next we can create the scales and make ethnicity and gender into factors. This is all identical to Study 1. (The one exception is that there were no Native American participants in this sample so that is not a level in the ethnicity factor)
```{r}
#_____________
#SDO
#reverse code items:
study2$SDO7_5 <- 8-study2$SDO7_5
study2$SDO7_6 <- 8-study2$SDO7_6
study2$SDO7_7 <- 8-study2$SDO7_7
study2$SDO7_8 <- 8-study2$SDO7_8
study2$SDO7_13 <- 8-study2$SDO7_13
study2$SDO7_14 <- 8-study2$SDO7_14
study2$SDO7_15 <- 8-study2$SDO7_15
study2$SDO7_16 <- 8-study2$SDO7_16

#creating the scale
study2$sdo <- study2 %>%
  select(starts_with("SDO7_")) %>%
  rowMeans(na.rm = TRUE) 

#checking for NAs--there shouldn't really be any bc everyone should have at least some SDO values in this 
anyNA(study2$sdo) #no NAs

#_____________
#ETHNIC ID 
#we want to use ethnic_id2 & ethnic_id3; those are the only ones in all studies.
study2$ethnic <- rowMeans(subset(study2, select = c(ethnic_id2, ethnic_id3)), na.rm = TRUE)
summary(study2$ethnic) #no NAs, great


#_____________
#FORMATIVENESS & CATEGORIZATION
#hypo 1-4 are categorization; hypo 4-5, 11-12 are formativeness (but not included)
#compute the scale scale
study2$form <- rowMeans(subset(study2, select = c(hypo_4, hypo_5, hypo_11, hypo_12), na.rm = TRUE))
study2$cat <- rowMeans(subset(study2, select = c(hypo_1, hypo_2), na.rm = TRUE))

#_____________
#MAKING ETHNIC ID & GENDER FACTORS
study2$ethnic_id <- factor(study2$ethnic_id, labels = c("Black/African American", "Asian/Asian American/Pacific Islander", 
                                                        "White", "Latino/Hispanic American", "Middle Eastern/Arab American",
                                                        "Biracial/Mixed race (please specify)", "Other (please specify)"))

study2$demo_gend2 <- factor(study2$demo_gend, labels = c("male", "female"))
```
Excellent! The data is ready for analysis.  

##Analysis Chunk 5: Study 1 Demographics  
```{r}
nrow(study2) #501 people total in study 2

#how many white participants?
study2 %>% 
  filter(ethnic_id == "White") %>%
  nrow() #411 White participants

#selecting white participants who completed teh attention checks; final sample
study2f <- study2 %>%
  filter(ethnic_id == "White" & attn == 5 & attn2 == 3) #making final study 1 dataset
nrow(study2f) #final sample is 404
411 - nrow(study2f) #7 people didn't pass attention checks

#mean age

#checking for outliers
study2f$demo_age[study2f$demo_age > 100 | study2f$demo_age < 18] #no outliers
describe(study2f$demo_age, na.rm = TRUE) #mean age is 35.90; sd = 12.08

#gender
summary(study2f$demo_gend2) #169 males, 235 females; 
169/(169+235) #41.8% females
```

##Analysis Chunk 6: Study 2 Scales  
```{r}
#ethnic id correlation
cor.test(study2f$ethnic_id2, study2f$ethnic_id3) #.71 p < .001

#sdo reliability
alpha(select(study2f, starts_with("SDO7_")))  #sdo reliability is .94

#formativeness alpha
alpha(subset(study2f, select = c(hypo_4, hypo_5, hypo_11,hypo_12))) #.77 p < .001
```
##Analysis Chunk 7: Study 2 Manipulation Check  
```{r}
s2manip <- aov(with(study2f, status ~ cond)) #this seems roughly equivalant to the t-test
summary(s2manip)
etaSquared(s2manip) #calculating the effect size; this matches spss

#getting the mean/sd for each condition
study2f %>%
  group_by(cond) %>%
  summarise(mean = mean(status, na.rm = T), sd = sd(status, na.rm = T))
```

##Analysis Chunk 8: Study 2 Three-Way Interaction
First, we mean-center our continuous variables and make sure our dichotomous condition variable is a factor:
```{r}
study2f$sdocent <- scale(study2f$sdo, center = TRUE, scale = FALSE)
study2f$ethcent <- scale(study2f$ethnic, center = TRUE, scale = FALSE)
study2f$sdocent <- study2f$sdocent[,1]
study2f$ethcent <- study2f$ethcent[,1]
study2f$cond2 <- as.factor(study2f$cond)
study2f$condcent <- scale(as.numeric(as.character(study2f$cond)), center= TRUE, scale = FALSE)
```
Now we can run our 3-way interaction between SDO, ethnic identification, and condition (including the simple slopes).
```{r}
s2model <- lm(form~sdocent*ethcent*cond2, data=study2f)
```
As before, we'll reshape these results so can be used more easily (in the context of the paper). 
```{r}
s2modeldf <- tidy(s2model) #pulling out regression results
s2cidf <- tidy(confint(s2model)) #pulling out CIs
names(s2cidf) <- c("parameter", "lowerci", "upperci") #renamingh

s2modeldf <- bind_cols(s2modeldf, s2cidf) #binding regression results and CIs
names(s2modeldf) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci") #and renaming

s2modeldf %>%
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>% #rounding
  select(parameter, B, p, lowerci, upperci) #selecting variables we need
```
Now we can do a similar thing with the simple slopes (but we'll have to manually compute the CIs again).
```{r}
s2simpslope <- simple_slopes(s2model) #pulling out the simple slopes to a df

s2simpsloper <- s2simpslope[c(8, 2, 17, 11, 6, 4, 20, 14), ] #pulling out relevant rows
names(s2simpsloper)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p") #renaming

s2simpsloper %>% 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding according to APA
  mutate(parameter = c("cond_at_high_eth", "cond_at_low_eth", "eth_in_high_cond",
                       "eth_in_low_cond", "cond_at_high_sdo","cond_at_low_sdo",  
                       "sdo_in_high_cond","sdo_in_low_cond")) %>% #making new parameter col
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```


#Study 3 Analyses
The set-up and analyses of Study 3 was virtually identical to Study 2. We begin by importing the data (again, labels and suspicion question were removed from the csv prior to this import). Then we filter out incomplete cases and people who did not consent to letting us use their data. 
```{r}
study3 <- read.csv(file="study 3 project artist no labels.csv")
#start by filtering out the cases where people didn't consent to letting us use their data
nrow(study3) # we start with 591 lines of data
study3 <- study3 %>%
  filter(data_yn == 1) #this filters out people who didn't complete the study or didn't let us use data
nrow(study3) #there are now 506 people in this sample 

```
##Cleaning the Data
Similar to prior studies, we next created a contrast-coded condition variable. 

```{r}
study3$cond[study3$DO.BR.FL_53 == "high status"] <- 1
study3$cond[study3$DO.BR.FL_115 == "high status"] <- 1
study3$cond[study3$DO.BR.FL_53 == "low Status"] <- -1
study3$cond[study3$DO.BR.FL_115 == "low Status"] <- -1

 #changing this to a factor
summary(study3$cond) #259 bad cond; 247 good cond

study3 %>%
  group_by(cond) %>%
  summarise('n' = n(), 'status' = mean(status, na.rm = TRUE)) #259 bad cond; 247 good cond#perceptions of status are higher in good condition; i.e., it's correctly coded
```
Next, we will create the SDO, ethnic identification, and formativeness and categorization DVs. We'll also make gender and ethnic identification into factors. 
```{r}
#SDO
#reverse code items:
study3$SDO7_5 <- 8-study3$SDO7_5
study3$SDO7_6 <- 8-study3$SDO7_6
study3$SDO7_7 <- 8-study3$SDO7_7
study3$SDO7_8 <- 8-study3$SDO7_8
study3$SDO7_13 <- 8-study3$SDO7_13
study3$SDO7_14 <- 8-study3$SDO7_14
study3$SDO7_15 <- 8-study3$SDO7_15
study3$SDO7_16 <- 8-study3$SDO7_16

#creating the scale
study3$sdo <- study3 %>%
  select(starts_with("SDO7_")) %>%
  rowMeans(na.rm = TRUE) 

#checking for NAs 
anyNA(study3$sdo) #no NAs

#_____________
#ETHNIC ID 
#we want to use ethnic_id2 & ethnic_id3; those are the only ones in all studies.
study3$ethnic <- rowMeans(subset(study3, select = c(ethnic_id2, ethnic_id3)), na.rm = TRUE)
summary(study3$ethnic) #no NAs, great

#_____________
#FORMATIVENESS & CATEGORIZATION
#hypo 1-4 are categorization; hypo 4-5, 11-12 are formativeness (but not included)
#compute the scale scale
study3$form <- rowMeans(subset(study3, select = c(hypo_4, hypo_5, hypo_11, hypo_12), na.rm = TRUE))
study3$cat <- rowMeans(subset(study3, select = c(hypo_1, hypo_2), na.rm = TRUE))

#_____________
#MAKING ETHNIC ID & GENDER FACTORS
study3$ethnic_id <- factor(study3$ethnic_id, labels = c("Black/African American", "Asian/Asian American/Pacific Islander", "White", "Latino/Hispanic American", "Middle Eastern/Arab American","Native American", "Biracial/Mixed race (please specify)", "Other (please specify)"))

study3$demo_gend2 <- factor(study3$demo_gend, labels = c("male", "female"))
```
Great! With that all done, we are ready to proceed to the Study 3 analyses.

##Analysis Chunk 9: Study 3 Demographics  
```{r}
#total number of people in Study 3 
nrow(study3) #506 people total in study 1

#how many white participants?
study3 %>% 
  filter(ethnic_id == "White") %>%
  nrow() #379 White participants

#selecting white participants who completed the attention checks; final sample
study3f <- study3 %>%
  filter(ethnic_id == "White" & attn == 5 & attn2 == 3) #making final study 1 dataset
nrow(study3f) #final sample is 368
379 - nrow(study3f) #11 people didn't pass attention checks


#ages
#checking for outliers
study3f$demo_age[study3f$demo_age > 100 | study3f$demo_age < 18] #no outliers
#looking for mean/standard deviation
describe(study3f$demo_age, na.rm = TRUE) #mean age is 38.80; sd = 13.29

#gender
summary(study3f$demo_gend2) #163 males, 205 females; 
205/(163+205) #55.7% females
```

##Analysis Chunk 10: Study 3 Scales  
```{r}
#ethnic id correlation
cor.test(study3f$ethnic_id2, study3f$ethnic_id3) #.77 p < .001

#sdo reliability
alpha(select(study3f, starts_with("SDO7_")))  #sdo reliability is .95

#formativeness alpha
alpha(subset(study3f, select = c(hypo_4, hypo_5, hypo_11,hypo_12))) #.77 p < .001

```


##Analysis Chunk 11: Study 3 Manipulation Check  
```{r}
s3manip <- aov(with(study3f, status ~ cond)) #building the anova model 
summary(s3manip)#extracting results from the model
etaSquared(s3manip) #calculating the effect size

#getting the mean/sd for each condition
study3f %>%
  group_by(cond) %>%
  summarise(mean = mean(status, na.rm = T), sd = sd(status, na.rm = T))


```

##Analysis Chunk 12: Study 3 Three-Way Interaction
First, we mean-centered our continuous variables and change our dichotomous condition variable as a factor:
```{r}
study3f$sdocent <- scale(study3f$sdo, center = TRUE, scale = FALSE)
study3f$ethcent <- scale(study3f$ethnic, center = TRUE, scale = FALSE)
study3f$sdocent <- study3f$sdocent[,1]
study3f$ethcent <- study3f$ethcent[,1]
study3f$cond2 <- as.factor(study3f$cond)
```
Now we can run our 3-way interaction between SDO, ethnic identification, and condition (including the simple slopes).
```{r}
s3model <- lm(form~sdocent*ethcent*cond2, data=study3f)
```
As before, we'll reshape these results so can be used more easily.
```{r}
s3modeldf <- tidy(s3model) #pulling out regression results
s3cidf <- tidy(confint(s3model)) #pulling out CIs
names(s3cidf) <- c("parameter", "lowerci", "upperci") #renaming

s3modeldf <- bind_cols(s3modeldf, s3cidf) #binding regression results and CIs
names(s3modeldf) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci") #and renaming

s3modeldf %>%
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>% #rounding
  select(parameter, B, p, lowerci, upperci) #selecting variables we need
```
Now we'll look at the simple slopes.
```{r}
s3simpslope <- simple_slopes(s3model) #pulling out the simple slopes to a df

s3simpsloper <- s3simpslope[c(8, 2, 17, 11, 6, 4, 20, 14), ] #pulling out relevant rows
names(s3simpsloper)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p") #renaming

s3simpsloper %>% 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding according to APA
  mutate(parameter = c("cond_at_high_eth", "cond_at_low_eth", "eth_in_high_cond",
                       "eth_in_low_cond", "cond_at_high_sdo","cond_at_low_sdo",  
                       "sdo_in_high_cond","sdo_in_low_cond")) %>% #making new parameter col
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```



#Analyses with Combined Dataset
We were also interested in looking at the effects of all of our studies combined. Given that our research design was extremely similar from study to study and we used all the same measures across studies, we decided to simply merge studies. 

##Merging Studies 1-3
Before we merge, we will create a new dataframe for each study that contains only the variables that we plan on using in the final analysis. Study 1 is already only a select subject of variables due to the Study 1a/Study 1b merge, so nothing needs to be selected out of it.

```{r}
#study 1 variables are already selected; need to rename our Study 1A/1B variable so it
#doesn't interfere with our new source variable
study1merge <- study1

#selecting variables for the merge from study 2
study2merge <- study2 %>%
  select(V1,demo_age, demo_gend, starts_with("ethnic"), attn, starts_with("SDO"), status, 
         attn2, starts_with("hypo"), cond, ethnic, sdo, form, cat, demo_gend2) #study2 is ready to go

#selecting variables for the merge from study 3
study3merge <- study3 %>%
  select(V1,demo_age, demo_gend, starts_with("ethnic"), attn, starts_with("SDO"), status, 
         attn2, starts_with("hypo"), cond, ethnic, sdo, form, cat) 
```
We also need to change a couple of the factors to character variables. If you have two factors of different levels, R won't let you merge them, so we avoid this by making the factors into character variables. 
```{r}
study1merge$cond<- as.character(study1merge$cond)
study2merge$cond<- as.character(study2merge$cond)
study3merge$cond<- as.character(study3merge$cond)

study1merge$ethnic_id <- as.character(study1merge$ethnic_id)
study2merge$ethnic_id <- as.character(study2merge$ethnic_id)
study3merge$ethnic_id <- as.character(study3merge$ethnic_id)
```
We will now merge the datasets. The new variable "source" will tell us which study the data came from.
```{r message = FALSE}
combined <- bind_rows("study1" = study1merge, "study2" = study2merge, "study3" = study3merge, .id = "source")


nrow(combined) #1628 rows, that's equal to 106 + 113 + 197 +205 
summary(combined$cond) #checking the conditions --801 bad, 827 good
#study 1: 303 bad, 318 good
#study 2: 239 bad cond; 262 good cond
#study 3: 259 bad cond; 247 good cond
#total should be 801 bad, 827 good; we're good

combined %>%
  group_by(cond) %>%
  summarise('n' = n(), status =mean(status, na.rm = TRUE)) #status is still higher in reputable condition, everything looks reputable
```
Our merge is now complete!  I am going to export this to excel for future reference and double checking. 
```{r}
#exporting this data to Excel.
write.csv(combined, file = "3 studies combined.csv")
```
Just a few more things to do before the data is really for analyses. First, I'm going to make ethnic_id back into a factor:
```{r}
combined$ethnic_id <- factor(combined$ethnic_id, labels = c("Black/African American", "Asian/Asian American/Pacific Islander", 
                         "Latino/Hispanic American", "Middle Eastern/Arab American","Native American",
                         "Biracial/Mixed race (please specify)", "Other", "White"))

combined %>%
  group_by(ethnic_id) %>%
  summarise(n())
```
Next, we'll rename the `combined` dataframe as `cdf` so it's slightly easier to run analyses on it
```{r}
cdf <- combined
```
And finally, before we moving onto to cleaning the data/performing analyses, I like to remove some of the data frames we made in process, just to keep the working environment tidy.
```{r}
rm(study1merge)
rm(study2merge)
rm(study3merge)
```
                        
##Analysis Chunk 9: Combined Studies Demographics  
```{r}
#total number of people in all studies 
nrow(cdf) #1628 participants total 

#how many white participants?
cdf %>% 
  filter(ethnic_id == "White") %>%
  nrow() #1260 White participants

#selecting white participants who completed the attention checks; final sample
cdff <- cdf %>%
  filter(ethnic_id == "White" & attn == 5 & attn2 == 3) #making final study 1 dataset
nrow(cdff) #final sample is 1231
1260 - nrow(cdff) #29 people didn't pass attention checks
```

## Analysis Chunk 14: Three-Way Interaction
First, we mean-center our continuous variables and change our dichotomous condition variable as a factor:
```{r}
cdff$sdocent <- scale(cdff$sdo, center = TRUE, scale = FALSE)
cdff$ethcent <- scale(cdff$ethnic, center = TRUE, scale = FALSE)
cdff$sdocent <- cdff$sdocent[,1]
cdff$ethcent <- cdff$ethcent[,1]
cdff$cond2 <- as.factor(cdff$cond)

```
Now we can run our 3-way interaction between SDO, ethnic identification, and condition (including the simple slopes).
```{r}
cdfmodel <- lm(form~sdocent*ethcent*cond2, data=cdff)
summary(cdfmodel)
confint(cdfmodel) #this gives us the 95% confidence intervals

```
As before, we'll reshape these results so can be used more easily.
```{r}
cdfmodeldf <- tidy(cdfmodel) #pulling out regression results
cdfcidf <- tidy(confint(cdfmodel)) #pulling out CIs
names(cdfcidf) <- c("parameter", "lowerci", "upperci") #renaming

cdfmodeldf <- bind_cols(cdfmodeldf, cdfcidf) #binding regression results and CIs
names(cdfmodeldf) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci") #and renaming

cdfmodeldf %>%
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>% #rounding
  select(parameter, B, p, lowerci, upperci) #selecting variables we need
```
Now we can do a similar thing with the simple slopes (but we'll have to manually compute the CIs).
```{r}
cdfsimpslope <- simple_slopes(cdfmodel) #pulling out the simple slopes to a df

cdfsimpsloper <- cdfsimpslope[c(8, 2, 17, 11, 6, 4, 20, 14), ] #pulling out relevant rows
names(cdfsimpsloper)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p") #renaming

cdfsimpsloper %>% 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding according to APA
  mutate(parameter = c("cond_at_high_eth", "cond_at_low_eth", "eth_in_high_cond",
                       "eth_in_low_cond", "cond_at_high_sdo","cond_at_low_sdo",  
                       "sdo_in_high_cond","sdo_in_low_cond")) %>% #making new parameter col
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```
And that's it for the analyses for the main text!  

## Analysis Chunk 15: Descriptive Statistics Table
```{r}
#STUDY 1

#N per condition
study1f %>%
  group_by(cond2) %>%
  summarise(n())

#mean and SD for key variables:
describe(study1f$sdo)
describe(study1f$ethnic)
describe(study1f$form)
describe(study1f$cat)


library(Hmisc) #I library and remove this package as I need it because otherwise it messes up my "describe()" function
study1cortest <-study1f[, c('cond','sdo', 'ethnic', 'form', 'cat')]
s1cor <- rcorr(as.matrix(study1cortest), type='pearson')
round(s1cor$r, 2)
round(s1cor$P, 3)
detach("package:Hmisc", unload=TRUE)  


#study 2
#N per condition
study2f %>%
  group_by(cond2) %>%
  summarise(n())


#mean and SD for key variables:
describe(study2f$sdo)
describe(study2f$ethnic)
describe(study2f$form)
describe(study2f$cat)


library(Hmisc)
study2cortest <-study2f[, c('cond','sdo', 'ethnic', 'form', 'cat')]
s2cor <- rcorr(as.matrix(study2cortest), type='pearson')
round(s2cor$r, 2)
round(s2cor$P, 3)
detach("package:Hmisc", unload=TRUE)  


##study 3
#N per condition
study3f %>%
  group_by(cond2) %>%
  summarise(n())


#mean and SD for key variables:
describe(study3f$sdo)
describe(study3f$ethnic)
describe(study3f$form)
describe(study3f$cat)


library(Hmisc)
study3cortest <-study3f[, c('cond','sdo', 'ethnic', 'form', 'cat')]
s3cor <- rcorr(as.matrix(study3cortest), type='pearson')
round(s3cor$r, 2)
round(s3cor$P, 3)
detach("package:Hmisc", unload=TRUE)  


##combined data
#N per condition
cdff %>%
  group_by(cond2) %>%
  summarise(n())


#mean and SD for key variables:
describe(cdff$sdo)
describe(cdff$ethnic)
describe(cdff$form)
describe(cdff$cat)


library(Hmisc)
cdfcortest <-cdff[, c('cond','sdo', 'ethnic', 'form', 'cat')]
cdfcor <- rcorr(as.matrix(cdfcortest), type='pearson')
round(cdfcor$r, 2)
round(cdfcor$P, 3)
detach("package:Hmisc", unload=TRUE)  

```

## Analysis Chunk 16: Post-test Results (Footnote 8)
```{r}
#reading in the data
pretest <- read.csv('pretest from qualtrics 2.csv')

#filtering out incomplete, non-white cases
pretestr <- pretest %>%
  filter(ethnic_id == 3) %>%
  filter(DO.BR.FL_57 != "FL_58|attention/suspicion check") %>%
  filter(DO.BR.FL_57 != 'FL_129|attention/suspicion check') %>%
  filter(DO.BR.FL_57 != "attention/suspicion check|FL_129") %>%
  filter(DO.BR.FL_57 != 'attention/suspicion check|FL_58')

nrow(pretestr) #224 participants

#reverse code items:
pretestr$SDO7_5 <- 8-pretestr$SDO7_5
pretestr$SDO7_6 <- 8-pretestr$SDO7_6
pretestr$SDO7_7 <- 8-pretestr$SDO7_7
pretestr$SDO7_8 <- 8-pretestr$SDO7_8
pretestr$SDO7_13 <- 8-pretestr$SDO7_13
pretestr$SDO7_14 <- 8-pretestr$SDO7_14
pretestr$SDO7_15 <- 8-pretestr$SDO7_15
pretestr$SDO7_16 <- 8-pretestr$SDO7_16

#creating SDO
pretestr$sdo <- pretestr %>%
  select(starts_with("SDO7_")) %>%
  rowMeans(na.rm = TRUE) 

#SDO alpha is .94
psych::alpha(select(pretestr, starts_with("SDO7_")))


#ALEX (HALF-LATINO, STUDY 1) ANALYSES
#checking on status:
describe(pretestr$Q90) #4.92 (1.11)

#making alex formativeness scale
pretestr$alex_form <- pretestr %>%
  select(Q98:Q104) %>%
  rowMeans(na.rm = T) #making alex formativeness scale
psych::alpha(select(pretestr, c(Q98:Q104))) #alpha = .94

describe(pretestr$alex_form) #mean of 3.54, sd of 1.06


#ZACK (HALF-ARAB, STUDY 2) ANALYSES
#checking on status:
describe(pretestr$Q67) #4.09 (.94)--not bad

#making zack formativeness scale
pretestr$zack_form <- pretestr %>%
  select(Q77:Q83) %>%
  rowMeans(na.rm = T) #making zack formativeness scale
psych::alpha(select(pretestr, c(Q77:Q83))) #alpha = .95

#mean of formativeness scale
describe(pretestr$zack_form) #mean of 3.83, sd of 1.11



#JORDAN (HALF-BLACK, STUDY 3) ANALYSES
#checking on status:
describe(pretestr$status) #4.05 (1.15)

#making jordon formativeness scale
pretestr$jordon_form <- pretestr %>%
  select(hypo_3:hypo_1) %>%
  rowMeans(na.rm = T) #making jordon formativeness scale
psych::alpha(select(pretestr, c(hypo_3:hypo_1))) #alpha = .95

describe(pretestr$jordon_form) #mean of 3.45, sd of 1.06
```
That concludes the analyses for the main text.

### Supplemental Materials Script

The first thing that we need to do is the run the analyses for the pilot and follow-up study from Study 1 separately.  We call these Study 1a and Study 1b. 

# Analyses for Study 1a
To run the appropriate analyses, we have to reduce the `study1a` dataframe to include only White participants who passed the attention check. 
```{r}
study1awhite<- study1afull %>%
  filter(ethnic_id == 3)
study1af <- study1awhite %>%
  filter(attn ==5 & attn2 == 3)
```
We will also need to create the appropriate scales. 
```{r}
#_____________
#SDO
#reverse code items:
study1af$SDO7_5 <- 8-study1af$SDO7_5
study1af$SDO7_6 <- 8-study1af$SDO7_6
study1af$SDO7_7 <- 8-study1af$SDO7_7
study1af$SDO7_8 <- 8-study1af$SDO7_8
study1af$SDO7_13 <- 8-study1af$SDO7_13
study1af$SDO7_14 <- 8-study1af$SDO7_14
study1af$SDO7_15 <- 8-study1af$SDO7_15
study1af$SDO7_16 <- 8-study1af$SDO7_16

#create the scale
study1af$sdo <- study1af %>%
  select(starts_with("SDO7_")) %>%
  rowMeans(na.rm = TRUE) 

#checking for NAs (empty cases)--there shouldn't really be any because everyone should have at least some SDO values in this 
anyNA(study1af$sdo) #false means no NAs

#_____________#ETHNIC ID 
#we want to use ethnic_id2 & ethnic_id3; those are the only ones in all studies.
study1af$ethnic <- rowMeans(subset(study1af, select = c(ethnic_id2, ethnic_id3)), na.rm = TRUE) #making scale
summary(study1$ethnic) #no NAs, great


#_____________
#FORMATIVENESS & CATEGORIZATION
#hypo 1 and 2 are categorization; hypo 4-5, 11-12 are formativeness (but 11 and 12 are not included in Study 1a so we are not looking at them in this study)
study1af$form <- rowMeans(subset(study1af, select = c(hypo_4, hypo_5), na.rm = TRUE))
study1af$cat <- rowMeans(subset(study1af, select = c(hypo_1, hypo_2), na.rm = TRUE))
summary(study1af$form) #no NAs, great
summary(study1af$cat) #no NAs, great


#_____________
#MAKING GENDER A FACTOR
study1af$demo_gend2 <- factor(study1af$demo_gend, labels = c("male", "female"))
```
Next, let's look at the demographics for this study
```{r}
#number of participants 
nrow(study1afull) #327 participants
nrow(study1awhite) #244 white participants
nrow(study1af) #240 white paritcipants who passed the attention check

# calculating gender distribution
study1af %>%
  group_by(demo_gend2) %>%
  summarise('n' = n(),'%' = n()/nrow(study1af))

#ages
#making a factor
study1af$demo_age  <- as.numeric(as.character(study1af$demo_age)) #this will throuh a warning because there is one invalid non-numeric response. That is ok because it will just turn it into an NA which is what we want. 

#checking for outliers
study1af$demo_age[study1af$demo_age > 100 | study1af$demo_age < 18]
#removing outlier
study1af$demo_age[study1af$demo_age > 100 | study1af$demo_age < 18] <- NA 

#calulating mean age
describe(study1af$demo_age, na.rm = TRUE) #mean age is 35.90; sd = 12.8

```
Now we can look at the 3-way interaction.  
First, we mean-center our continuous variables. We want the neutral category to be our reference category.  In order to do this, we are going to recode the condition variable so that the neutral category has the smallest value. We then make it a factor.
```{r}
study1af$sdocent <- scale(study1af$sdo, center = TRUE, scale = FALSE)
study1af$ethcent <- scale(study1af$ethnic, center = TRUE, scale = FALSE)
study1af$sdocent <- study1af$sdocent[,1]
study1af$ethcent <- study1af$ethcent[,1]
#making the 'neutral' category the reference category.
study1af$cond2 <- ifelse(study1af$condition == 2, 0,
                         ifelse(study1af$condition == 1, 1, 2))
study1af$cond2 <- as.factor(study1af$cond2)
levels(study1af$cond2)
```
Now we can run our 3-way interaction between SDO, ethnic identification, and condition (including the simple slopes).
```{r}
s1amodel <- lm(form~sdocent*ethcent*cond2, data=study1af) 
summary(s1amodel)
confint(s1amodel) #this gives us the 95% confidence intervals
```
As before, we'll reshape these results so they can be used more easily.
      
```{r}
s1amodeldf <- tidy(s1amodel) #pulling out regression results
s1amodeldf
s1acidf <- tidy(confint(s1amodel)) #pulling out CIs
names(s1acidf) <- c("parameter", "lowerci", "upperci") #renamingh
s1amodeldf
s1acidf

s1amodeldf <- bind_cols(s1amodeldf, s1acidf) #binding regression results and CIs
names(s1amodeldf) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci") #and renaming

s1amodeldf %>%
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>% #rounding
  select(parameter, B, p, lowerci, upperci) #selecting variables we need
```
That's great. We also need to find out the results that compare the disreputable condition with the reputable condition.
```{r}
study1af$cond3 <- ifelse(study1af$cond2 == 0, 1, ifelse(study1af$cond2==1, 0, 2))
study1af$cond3 <- as.factor(study1af$cond3)
levels(study1af$cond3)
#verifying that this worked correctly
  study1af %>%
  group_by(cond3) %>%
  summarise(n())

study1af %>%
  group_by(cond2) %>%
  summarise(n())

#yes, run model
s1amodel2 <- lm(form~sdocent*ethcent*cond3, data=study1af) 
summary(s1amodel2)
confint(s1amodel2)
simple_slopes(s1amodel2)

s1amodel2df <- tidy(s1amodel2) #pulling out regression results
s1aci2df <- tidy(confint(s1amodel2)) #pulling out CIs
names(s1aci2df) <- c("parameter", "lowerci", "upperci") #renamingh


s1amodel2df <- bind_cols(s1amodel2df, s1aci2df) #binding regression results and CIs
names(s1amodel2df) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci") #and renaming

s1amodel2df %>%
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>% #rounding
  select(parameter, B, p, lowerci, upperci) #selecting variables we need

write.csv(study1af, "study_1a.csv", row.names = F)
```

Now we can need to get the simple slopes (for both of the models above).
```{r}
#first set of condition level comparisons
s1asimpslope <- simple_slopes(s1amodel) #pulling out the simple slopes to a df
print(s1asimpslope)


s1asimpsloper <- s1asimpslope[c(16, 15,	4, 3, 32,	20,	26,	35,	23,	29,	12,	11,	8, 7), ] #pulling out relevant rows
names(s1asimpsloper)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p") #renaming


s1asimpsloper %>% 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding according to APA
  mutate(parameter = c("cond_neut_v_high_at_high_eth","cond_neut_v_low_at_high_eth",
                       "cond_neut_v_high_at_low_eth","cond_neut_v_low_at_low_eth",
                       "eth_in_high_cond", "eth_in_neut_cond", "eth_in_low_cond",
                       "sdo_in_high_cond",  "sdo_in_neut_cond", "sdo_in_low_cond", 
                       "cond_neut_v_high_at_high_sdo","cond_neut_v_low_at_high_sdo",
                       "cond_neut_v_high_at_low_sdo","cond_neut_v_low_at_low_sdo")) %>% #making new parameter col
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need

```
Now we can do the same thing for the second model, which compares the disreputable condition to the reputable condition. It will be abbreviated, though, because we are only looking at four comparisons.
```{r}
#first set of condition level comparisons
s1a2simpslope <- simple_slopes(s1amodel2) #pulling out the simple slopes to a df


s1a2simpsloper <- s1a2simpslope[c(16, 4, 12, 8), ] #pulling out relevant rows
names(s1a2simpsloper)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p") #renaming


s1a2simpsloper %>% 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding according to APA
  mutate(parameter = c("cond_low_v_high_at_high_eth",
                       "cond_low_v_high_at_low_eth",
                       "cond_low_v_high_at_high_sdo",
                       "cond_low_v_high_at_low_sdo")) %>% #making new parameter col
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need

```


# Analyses for Study 1b
Now we can do the same thing but for Study 1b. 
```{r}
study1bwhite <- study1b %>%
  filter(ethnic_id == 3)
study1bf <- study1bwhite %>%
  filter(attn ==5 & attn2 == 3)

```
We will also need to create the appropriate scales. 
```{r}
#_____________
#SDO
#reverse code items:
study1bf$SDO7_5 <- 8-study1bf$SDO7_5
study1bf$SDO7_6 <- 8-study1bf$SDO7_6
study1bf$SDO7_7 <- 8-study1bf$SDO7_7
study1bf$SDO7_8 <- 8-study1bf$SDO7_8
study1bf$SDO7_13 <- 8-study1bf$SDO7_13
study1bf$SDO7_14 <- 8-study1bf$SDO7_14
study1bf$SDO7_15 <- 8-study1bf$SDO7_15
study1bf$SDO7_16 <- 8-study1bf$SDO7_16

#create the scale
study1bf$sdo <- study1bf %>%
  select(starts_with("SDO7_")) %>%
  rowMeans(na.rm = TRUE) 

#checking for NAs (empty cases)--there shouldn't really be any because everyone should have at least some SDO values in this 
anyNA(study1bf$sdo) #false means no NAs

#_____________#ETHNIC ID 
#we want to use ethnic_id2 & ethnic_id3; those are the only ones in all studies.
study1bf$ethnic <- rowMeans(subset(study1bf, select = c(ethnic_id2, ethnic_id3)), na.rm = TRUE) #making scale
summary(study1$ethnic) #no NAs, great


#_____________
#FORMATIVENESS & CATEGORIZATION
#hypo 1 and 2 are categorization; hypo 4-5, 11-12 are formativeness (but 11 and 12 are not included in Study 1b so we are not looking at them in this study)
study1bf <- study1bf %>%
  rename(hypo_1 = Q78, hypo_2 = Q80) #renaming these variables
study1bf$form <- rowMeans(subset(study1bf, select = c(hypo_4, hypo_5), na.rm = TRUE))
study1bf$cat <- rowMeans(subset(study1bf, select = c(hypo_1, hypo_2), na.rm = TRUE))
summary(study1bf$form) #no NAs, great
summary(study1bf$cat) #no NAs, great

#_____________
#MAKING GENDER A FACTOR
study1bf$demo_gend2 <- factor(study1bf$demo_gend, labels = c("male", "female"))
```
Now for demographics 
```{r}
#number of participants 
nrow(study1b) #402 participants
nrow(study1bwhite) #313 white participants
nrow(study1bf) #305 white paritcipants who passed the attention check

# calculating gender distribution
study1bf %>%
  group_by(demo_gend2) %>%
  summarise('n' = n(),'%' = n()/nrow(study1bf))

#ages
#making a factor
study1bf$demo_age  <- as.numeric(as.character(study1bf$demo_age)) #this will throuh a warning because there is one invalid non-numeric response. That is ok because it will just turn it into an NA which is what we want. 

#checking for outliers
study1bf$demo_age[study1bf$demo_age > 100 | study1bf$demo_age < 18]
#there are no outliers

#calulating mean age
describe(study1bf$demo_age, na.rm = TRUE) #mean age is 35.90; sd = 12.8

```
Now we can look at the 3-way interaction.

First, we mean-centered our continuous variables and change our dichotomous condition variable as a factor:
```{r}
study1bf$sdocent <- scale(study1bf$sdo, center = TRUE, scale = FALSE)
study1bf$ethcent <- scale(study1bf$ethnic, center = TRUE, scale = FALSE)
study1bf$sdocent <- study1bf$sdocent[,1]
study1bf$ethcent <- study1bf$ethcent[,1]
study1bf$cond2 <- as.factor(study1bf$cond)
```
Now we can run our 3-way interaction between SDO, ethnic identification, and condition (including the simple slopes).
```{r}
s1bmodel <- lm(form~sdocent*ethcent*cond2, data=study1bf)
```
We'll reshape these results so can be used more easily.
```{r}
s1bmodeldf <- tidy(s1bmodel) #pulling out regression results
s1bcidf <- tidy(confint(s1bmodel)) #pulling out CIs
names(s1bcidf) <- c("parameter", "lowerci", "upperci") #renamingh

s1bmodeldf <- bind_cols(s1bmodeldf, s1bcidf) #binding regression results and CIs
names(s1bmodeldf) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci") #and renaming

s1bmodeldf %>%
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>% #rounding
  select(parameter, B, p, lowerci, upperci) #selecting variables we need
```
Now we can do a similar thing with the simple slopes (but we'll have to manually compute the CIs again).
```{r}
s1bsimpslope <- simple_slopes(s1bmodel) #pulling out the simple slopes to a df

s1bsimpsloper <- s1bsimpslope[c(8, 2, 17, 11, 6, 4, 20, 14), ] #pulling out relevant rows
names(s1bsimpsloper)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p") #renaming

s1bsimpsloper %>% 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding according to APA
  mutate(parameter = c("cond_at_high_eth", "cond_at_low_eth", "eth_in_high_cond",
                       "eth_in_low_cond", "cond_at_high_sdo","cond_at_low_sdo",  
                       "sdo_in_high_cond","sdo_in_low_cond")) %>% #making new parameter col
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need

```

#Study 1 Additional Analyses
First, results for the SDO x condition only (without ethnic identification).

```{r}
s1model_supp1 <- lm(form~sdocent*cond2, data=study1f) #creating model
s1modeldf_supp1 <- tidy(s1model_supp1) #making a df of results
s1cidf_supp1 <- tidy(confint(s1model_supp1)) #making a df of confidence intervals
names(s1cidf_supp1) <- c("parameter", "lowerci", "upperci")

s1modeldf_supp1 <- bind_cols(s1modeldf_supp1, s1cidf_supp1) #binding those dfs
names(s1modeldf_supp1) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")


describe(study1f$form)
describe(study2f$form)
describe(study3f$form)
describe(cdf$form)


summary(s1model_supp1)

describe(study1f$cat)
describe(study2f$cat)
describe(study3f$cat)
describe(cdf$cat)

s1modeldf_supp1 %>% #select variables, rounding, etc. 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need and reshape

#now for simple effects
s1simpslope_supp1 <- simple_slopes(s1model_supp1)

s1simpsloper_supp1 <- s1simpslope_supp1[c(3, 1, 5, 4), ] 
names(s1simpsloper_supp1)[c(3, 4, 5, 7)] <- c("estimate", "std_error", "t", "p")

s1simpsloper_supp1 %>% #adding confidence intervals
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_sdo","cond_at_low_sdo",
                       "sdo_in_high_cond","sdo_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```

And now looking at ethnic identification without the SDO control:
```{r}
s1model_supp1a <- lm(form~ethcent*cond2, data=study1f) #creating model
s1modeldf_supp1a <- tidy(s1model_supp1a) #making a df of results
s1cidf_supp1a <- tidy(confint(s1model_supp1a)) #making a df of confidence intervals
names(s1cidf_supp1a) <- c("parameter", "lowerci", "upperci")

s1modeldf_supp1a <- bind_cols(s1modeldf_supp1a, s1cidf_supp1a) #binding those dfs
names(s1modeldf_supp1a) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

s1modeldf_supp1a %>% #select variables, rounding, etc. 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need and reshape

#now for simple effects
s1simpslope_supp1a <- simple_slopes(s1model_supp1a)

s1simpsloper_supp1a <- s1simpslope_supp1a[c(3, 1, 5, 4), ] 
names(s1simpsloper_supp1a)[c(3, 4, 5, 7)] <- c("estimate", "std_error", "t", "p")

s1simpsloper_supp1a %>% #adding confidence intervals
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_eth","cond_at_low_eth",
                       "eth_in_high_cond","eth_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```
The other additional analysis we'll do with Study 1 is to look at categorization as a dependent variable.

```{R}
s1model_supp2 <- lm(cat~sdocent*ethcent*cond2, data=study1f)
s1modeldf_supp2 <- tidy(s1model_supp2) #pull out results
s1cidf_supp2 <- tidy(confint(s1model_supp2)) #pull out CIs
names(s1cidf_supp2) <- c("parameter", "lowerci", "upperci")
summary(s1model_supp2)
s1modeldf_supp2 <- bind_cols(s1modeldf_supp2, s1cidf_supp2) #binding the dataframe
names(s1modeldf_supp2) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

s1modeldf_supp2 %>% #rounding 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need



#adding the simple slopes
s1simpslope_supp2 <- simple_slopes(s1model_supp2)

s1simpsloper_supp2 <- s1simpslope_supp2[c(8, 2, 17, 11, 6, 4, 20, 14), ] #pulling out relevant rows
names(s1simpsloper_supp2)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p")

s1simpsloper_supp2 %>% #restucturing, rounding, etc. 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_eth", "cond_at_low_eth", "eth_in_high_cond",
                       "eth_in_low_cond", "cond_at_high_sdo","cond_at_low_sdo",
                       "sdo_in_high_cond","sdo_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```
#Study 2 Additional Analyses
First, results for the SDO x condition only (without ethnic identification):
```{r}
s2model_supp1 <- lm(form~sdocent*cond2, data=study2f) #creating model
s2modeldf_supp1 <- tidy(s2model_supp1) #making a df of results
s2cidf_supp1 <- tidy(confint(s2model_supp1)) #making a df of confidence intervals
names(s2cidf_supp1) <- c("parameter", "lowerci", "upperci")

s2modeldf_supp1 <- bind_cols(s2modeldf_supp1, s2cidf_supp1) #binding those dfs
names(s2modeldf_supp1) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

s2modeldf_supp1 %>% #select variables, rounding, etc. 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need and reshape

#now for simple effects
s2simpslope_supp1 <- simple_slopes(s2model_supp1)


s2simpsloper_supp1 <- s2simpslope_supp1[c(3, 1, 5, 4), ] 
names(s2simpsloper_supp1)[c(3, 4, 5, 7)] <- c("estimate", "std_error", "t", "p")

s2simpsloper_supp1 %>% #adding confidence intervals
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_sdo","cond_at_low_sdo",
                       "sdo_in_high_cond","sdo_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```
And now ethnic identification x condition on formativeness:
```{r}
s2model_supp1a <- lm(form~ethcent*cond2, data=study2f) #creating model
s2modeldf_supp1a <- tidy(s2model_supp1a) #making a df of results
s2cidf_supp1a <- tidy(confint(s2model_supp1a)) #making a df of confidence intervals
names(s2cidf_supp1a) <- c("parameter", "lowerci", "upperci")


s2modeldf_supp1a <- bind_cols(s2modeldf_supp1a, s2cidf_supp1a) #binding those dfs
names(s2modeldf_supp1a) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

s2modeldf_supp1a %>% #select variables, rounding, etc. 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need and reshape

#now for simple effects
s2simpslope_supp1a <- simple_slopes(s2model_supp1a)

s2simpsloper_supp1a <- s2simpslope_supp1a[c(3, 1, 5, 4), ] 
names(s2simpsloper_supp1a)[c(3, 4, 5, 7)] <- c("estimate", "std_error", "t", "p")

s2simpsloper_supp1a %>% #adding confidence intervals
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_eth","cond_at_low_eth",
                       "eth_in_high_cond","eth_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```

And now we'll look at categorization as the dependent variable in Study 2.

```{R}
s2model_supp2 <- lm(cat~sdocent*ethcent*cond2, data=study2f)
s2modeldf_supp2 <- tidy(s2model_supp2) #pull out results
s2cidf_supp2 <- tidy(confint(s2model_supp2)) #pull out CIs
names(s2cidf_supp2) <- c("parameter", "lowerci", "upperci")

s2modeldf_supp2 <- bind_cols(s2modeldf_supp2, s2cidf_supp2) #binding the dataframe
names(s2modeldf_supp2) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

s2modeldf_supp2 %>% #rounding 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need

#adding the simple slopes
s2simpslope_supp2 <- simple_slopes(s2model_supp2)

s2simpsloper_supp2 <- s2simpslope_supp2[c(8, 2, 17, 11, 6, 4, 20, 14), ] #pulling out relevant rows
names(s2simpsloper_supp2)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p")

s2simpsloper_supp2 %>% #restucturing, rounding, etc. 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_eth", "cond_at_low_eth", "eth_in_high_cond",
                       "eth_in_low_cond", "cond_at_high_sdo","cond_at_low_sdo",
                       "sdo_in_high_cond","sdo_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```
#Study 3 Additional Analyses
Like above, we'll start with the results for the SDO x condition only (without ethnic identification). 
```{r}
s3model_supp1 <- lm(form~sdocent*cond2, data=study3f) #creating model
s3modeldf_supp1 <- tidy(s3model_supp1) #making a df of results
s3cidf_supp1 <- tidy(confint(s3model_supp1)) #making a df of confidence intervals
names(s3cidf_supp1) <- c("parameter", "lowerci", "upperci")

s3modeldf_supp1 <- bind_cols(s3modeldf_supp1, s3cidf_supp1) #binding those dfs
names(s3modeldf_supp1) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

s3modeldf_supp1 %>% #select variables, rounding, etc. 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need and reshape

#now for simple effects
s3simpslope_supp1 <- simple_slopes(s3model_supp1)

s3simpsloper_supp1 <- s3simpslope_supp1[c(3, 1, 5, 4), ] 
names(s3simpsloper_supp1)[c(3, 4, 5, 7)] <- c("estimate", "std_error", "t", "p")

s3simpsloper_supp1 %>% #adding confidence intervals
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_sdo","cond_at_low_sdo",
                       "sdo_in_high_cond","sdo_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```
Now ethnic identification by condition interaction:
```{r}
s3model_supp1a <- lm(form~ethcent*cond2, data=study3f) #creating model
s3modeldf_supp1a <- tidy(s3model_supp1a) #making a df of results
s3cidf_supp1a <- tidy(confint(s3model_supp1a)) #making a df of confidence intervals
names(s3cidf_supp1a) <- c("parameter", "lowerci", "upperci")

s3modeldf_supp1a <- bind_cols(s3modeldf_supp1a, s3cidf_supp1a) #binding those dfs
names(s3modeldf_supp1a) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

s3modeldf_supp1a %>% #select variables, rounding, etc. 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need and reshape

#now for simple effects
s3simpslope_supp1a <- simple_slopes(s3model_supp1a)

s3simpsloper_supp1a <- s3simpslope_supp1a[c(3, 1, 5, 4), ] 
names(s3simpsloper_supp1a)[c(3, 4, 5, 7)] <- c("estimate", "std_error", "t", "p")

s3simpsloper_supp1a %>% #adding confidence intervals
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_eth","cond_at_low_eth",
                       "eth_in_high_cond","eth_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```

And then Study 3 with categorization as a dependent variable.
```{R}
s3model_supp2 <- lm(cat~sdocent*ethcent*cond2, data=study3f)
s3modeldf_supp2 <- tidy(s3model_supp2) #pull out results
s3cidf_supp2 <- tidy(confint(s3model_supp2)) #pull out CIs
names(s3cidf_supp2) <- c("parameter", "lowerci", "upperci")

s3modeldf_supp2 <- bind_cols(s3modeldf_supp2, s3cidf_supp2) #binding the dataframe
names(s3modeldf_supp2) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

s3modeldf_supp2 %>% #rounding 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need

#adding the simple slopes
s3simpslope_supp2 <- simple_slopes(s3model_supp2)

s3simpsloper_supp2 <- s3simpslope_supp2[c(8, 2, 17, 11, 6, 4, 20, 14), ] #pulling out relevant rows
names(s3simpsloper_supp2)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p")

s3simpsloper_supp2 %>% #restucturing, rounding, etc. 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_eth", "cond_at_low_eth", "eth_in_high_cond",
                       "eth_in_low_cond", "cond_at_high_sdo","cond_at_low_sdo",
                       "sdo_in_high_cond","sdo_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```

#Combined Data Additional Analyses
First, results for the SDO x condition only (without ethnic identification). 
```{r}
cdmodel_supp1 <- lm(form~sdocent*cond2, data=cdff) #creating model
cdmodeldf_supp1 <- tidy(cdmodel_supp1) #making a df of results
cdcidf_supp1 <- tidy(confint(cdmodel_supp1)) #making a df of confidence intervals
names(cdcidf_supp1) <- c("parameter", "lowerci", "upperci")

cdmodeldf_supp1 <- bind_cols(cdmodeldf_supp1, cdcidf_supp1) #binding those dfs
names(cdmodeldf_supp1) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

cdmodeldf_supp1 %>% #select variables, rounding, etc. 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need and reshape

#now for simple effects
cdsimpslope_supp1 <- simple_slopes(cdmodel_supp1)

cdsimpsloper_supp1 <- cdsimpslope_supp1[c(3, 1, 5, 4), ] 
names(cdsimpsloper_supp1)[c(3, 4, 5, 7)] <- c("estimate", "std_error", "t", "p")

cdsimpsloper_supp1 %>% #adding confidence intervals
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_sdo","cond_at_low_sdo",
                       "sdo_in_high_cond","sdo_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```
And now we'll get the results for ethnic identification interaction without SDO.
```{r}
cdmodel_supp1a <- lm(form~ethcent*cond2, data=cdff) #creating model
cdmodeldf_supp1a <- tidy(cdmodel_supp1a) #making a df of results
cdcidf_supp1a <- tidy(confint(cdmodel_supp1a)) #making a df of confidence intervals
names(cdcidf_supp1a) <- c("parameter", "lowerci", "upperci")

cdmodeldf_supp1a <- bind_cols(cdmodeldf_supp1a, cdcidf_supp1a) #binding those dfs
names(cdmodeldf_supp1a) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

cdmodeldf_supp1a %>% #select variables, rounding, etc. 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need and reshape

#now for simple effecta
cdsimpslope_supp1a <- simple_slopes(cdmodel_supp1a)

cdsimpsloper_supp1a <- cdsimpslope_supp1a[c(3, 1, 5, 4), ] 
names(cdsimpsloper_supp1a)[c(3, 4, 5, 7)] <- c("estimate", "std_error", "t", "p")

cdsimpsloper_supp1a %>% #adding confidence intervals
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_eth","cond_at_low_eth",
                       "eth_in_high_cond","eth_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we need
```
The other additional analysis we'll do with Study 3 is to look at categorization as a dependent variable.
```{R}
cdmodel_supp2 <- lm(cat~sdocent*ethcent*cond2, data=cdff)
cdmodeldf_supp2 <- tidy(cdmodel_supp2) #pull out results
cdcidf_supp2 <- tidy(confint(cdmodel_supp2)) #pull out CIs
names(cdcidf_supp2) <- c("parameter", "lowerci", "upperci")

cdmodeldf_supp2 <- bind_cols(cdmodeldf_supp2, cdcidf_supp2) #binding the dataframe
names(cdmodeldf_supp2) <- c("term", "estimate", "std_err", "stat", "p", "parameter", "lowerci", "upperci")

cdmodeldf_supp2 %>% #rounding 
  mutate(B = round(estimate, 2), p = round(p, 3), lowerci = round(lowerci, 2), upperci = round(upperci, 2)) %>%
  select(parameter, B, p, lowerci, upperci) #select variables we need

#adding the simple slopes
cdsimpslope_supp2 <- simple_slopes(cdmodel_supp2)

cdsimpsloper_supp2 <- cdsimpslope_supp2[c(8, 2, 17, 11, 6, 4, 20, 14), ] #pulling out relevant rows
names(cdsimpsloper_supp2)[c(4, 5, 6, 8)] <- c("estimate", "std_error", "t", "p")

cdsimpsloper_supp2 %>% #restucturing, rounding, etc. 
  mutate(lower95ci = round(estimate-(2*std_error), 2), upper95ci = round(estimate+(2*std_error), 2)) %>% #creating CI values
  mutate(estimate = round(estimate, 2), p = round(p, 3)) %>% #rounding
  mutate(parameter = c("cond_at_high_eth", "cond_at_low_eth", "eth_in_high_cond",
                       "eth_in_low_cond", "cond_at_high_sdo","cond_at_low_sdo",
                       "sdo_in_high_cond","sdo_in_low_cond")) %>%  #making new parameter column
  select(parameter, estimate, p, ends_with("95ci")) #selecting only the variables we ne

```
And that's it!
